# LBRAG:基于？？的多语言RAGシステムの构筑

## 研究动机

在全球化信息环境下，大量知识以英文存在，但用户常使用本地语言（如日语等）提出问题。如果**检索增强生成（RAG）**系统只能处理英文，那么非英语用户将难以获取准确答案。这推动我们研究**多语言RAG**：让系统面对非英语提问时，也能检索英文或多语言知识，并用提问语言作答。这一课题具有重要意义：一方面，可以弥合语言壁垒，提升低资源语言用户获取知识的能力；另一方面，可提高现有大型语言模型在非英语场景下的准确性和可信度。

目前RAG在英语环境下已证明有效，但在多语言场景中的应用仍属新兴领域。简单的翻译方案存在局限：例如将用户问题翻译成英语再检索虽然方便，但可能遗漏原语言独有的信息。直接采用多语言检索则能覆盖更多文档，却可能因不同语言检索结果内容不一致而影响生成答案的连贯和正确性。现实应用中常出现跨语言情形，例如全球通用的问答系统可能仅有英文知识库却服务各国语言用户，或需要将本地语言资料与英文资料结合以回答本地化问题。有研究发现，仅用英文或仅用本地语言检索都不如二者结合效果好。因此，有必要探索新的多语言RAG架构，以同时实现**跨语言高召回**和**答案生成语言一致**的目标。

XRAG: Cross-lingual Retrieval-Augmented Generation一文证明：把英文知识库直接迁移到全球用户并不可靠；即使给模型双语文档，真正难题仍是 跨语信息如何齐心协作。未来的多语言 RAG 系统必须同时解决 回答语言一致性 和 跨语推理，XRAG 为衡量这一目标提供了新的、具有挑战性的标尺。

## 核心研究问题

本课题拟重点解决以下核心问题：

* \*\*跨语言检索策略：\*\*在用户使用非英语提问时，哪种检索策略效果最佳？需比较*翻译查询检索*（将问题翻译成英语检索）、*多语言检索*（直接在多语言语料中检索）与*跨语言检索融合*（结合多语言检索并统一语言处理）的性能。哪个策略能在保证高覆盖率的同时避免因语言差异导致的信息不一致？

* **答案语言一致性：**如何确保系统的回答语言与提问语言一致，并避免**代码混用**（如回答中夹杂英文片段）？现有研究指出，跨语言RAG常出现回答语言不正确的问题。我们需要探索利用提示词约束、语言检测等方法，保证生成答案完全使用用户语言。

* **跨语言知识整合：**当检索结果包含多种语言的信息时，如何让生成模型**正确整合**不同语言的证据进行推理？这一跨语言推理被认为是多语言RAG面临的主要挑战之一。本研究需要设计机制（例如将证据翻译到同一语言后再生成）来降低因证据语言差异带来的推理困难。

* \*\*性能提升与适用性：\*\*如何利用现有多语言预训练模型和翻译工具提升RAG性能？例如，多语言嵌入检索模型是否较简单翻译检索有显著优势？对低资源语言，跨语言RAG能否显著优于仅用单语资料检索？

* **评估指标与方法：**在多语言场景下，如何客观评估检索和生成性能？需要考虑回答内容准确度（如精确匹配EM、F1值）、流畅度和正确性，以及**语言一致性**等指标。例如，是否需要引入BLEU等自动评价指标评估答案与参考答案的匹配程度？评价时还需注意多语言下拼写和实体名称差异，对现有指标进行调整。如何检测回答中的语言混杂现象，并将其纳入评价标准？

## 本研究系统的设计要点与创新之处

为提升跨语言RAG性能，我们将设计一个**原创的多语言RAG系统架构**，融合多种策略并针对上述问题提出改进：

* **双通道检索与跨语言聚合：**系统包含**多语言检索**组件，利用先进的多语言向量检索模型（如BGE-m3等）在包含多种语言的知识库中搜索答案片段。同时引入**查询翻译检索**作为补充，即将非英语问题翻译为英语，在英文知识库中检索。当用户提问语言资源匮乏时，翻译检索可提供额外覆盖。检索阶段将返回用户语言和英语的候选证据集合。

* **证据统一与跨语言理解：**针对不同语言的检索结果，我们的系统将采用**跨语言对齐融合**策略。具体而言，先对检索到的证据进行语言识别，然后**将证据翻译成统一语言**（例如英文）以供生成模型使用。这种方法类似于CrossRAG提出的将文档翻译成公共语言，再生成答案的做法，可减少因证据语言不一致导致的理解偏差。与直接多语言生成相比，此步骤确保生成模型能在单一语言上下文中充分利用证据，提升推理准确性。

* **多语言生成与语言控制：**生成阶段采用**多语言大模型**（如mT5、LLaMA-2多语言版等）作为基础。我们将精心设计提示（Prompt）来控制生成语言：模型输入将包括统一语言的证据和原始问题（保留原语言或翻译后附加原问作为参考），并明确指示模型“用XX语言回答”。这样的**显式指令**可有效避免模型输出英文等非预期语言，提高回答语言的正确率。此外，可结合**少样本示例**提示模型如何只用目标语言表述事实。例如，对日语提问，提供示例展示如何引用英文资料但用日语作答，从而减少回答中的语言混用现象。

* **答案一致性检查（可选创新）：**为进一步保证回答准确和一致，我们考虑在生成后增加**自我检查**步骤。模型生成初步答案后，可以将答案翻译成另一语言（例如英文）与原证据进行比对，检查关键事实是否一致。如果发现偏差，模型可通过**自反思**机制修改答案。这一想法受Self-RAG启发，即让模型对自身生成进行审视和调整以提高 factuality。在多语言场景下，简单的自我检查（如将答案译回英文核对）可充当轻量的“事实核验”，提高最终答案的可靠性。（该部分为拓展思路，可视实际实现情况决定采用程度。）

* **优化检索排序与过滤：**在检索结果融合阶段，引入**跨语言重排序**策略。我们将证据统一语言后，根据与查询的语义相关度重新排序，并**过滤无关或冲突证据**。例如，如果某些证据段内容与其他段矛盾或不相关，可予以剔除，降低干扰。这可通过判别模型或简单启发式（比如根据证据中是否包含可能答案实体）实现。此举旨在确保生成阶段接收到的上下文高相关且一致，减少生成错误依据不良证据的可能性。

* **针对低资源语言的增强：**对于检索结果中涉及低资源语言的内容，我们计划结合**机器翻译和回译增强**技术。一方面，将低资源语言证据译成高资源语言供模型阅读；另一方面，若答案需要包含该语言的专有名词，我们可以利用双语词典或翻译工具将英文答案中的关键术语转译为目标语言术语，保证答案用词准确。通过这种方法，既利用英文信息提升回答内容完整性，又确保最终答案地道贴合用户语言习惯。

本系统设计的**创新点**在于融合现有方法的优点并补足其缺陷：(1) 相比纯粹的查询翻译，我们通过多语言检索扩大了信息覆盖面，不会漏掉本地语言独有知识；(2) 相比直接多语言检索，我们增加了证据翻译整合步骤，**统一了生成所用的知识语言环境**，提高了跨语言内容一致性；(3) 利用明确的生成指令和提示技术，解决了模型回答语言紊乱的问题，大幅减少回答中夹杂错误语言的情况；(4) 通过自检和重排序等机制，进一步提高答案的事实准确性和语言地道度。整体而言，该架构有望显著提升跨语言开放域问答的效果，在高资源与低资源语言上都表现出色。

## 可参考的系统与论文

为设计和评估本研究的系统，我们调研了近年提出的相关多语言RAG架构和系统，选取以下几个具有代表性的方案作为参考对象，并将在实验中与之对比分析：

* **问题翻译检索 (Translate-Query RAG)**：《Multilingual Retrieval-Augmented Generation for Knowledge-Intensive Task》提出将非英文**查询翻译**成英文，再使用英文检索模型获取答案。这一策略利用了丰富的英文知识库和成熟的英文检索技术，实现简单直接。然而，其局限在于**覆盖率不足**：如果某些信息仅存在于非英文资料中，翻译查询就无法检索到。因此该方法在涉及本地知识的问答上可能表现不佳。

* **多语言直接检索 (MultiRAG)**：不经过翻译，直接使用多语言检索模型从多语言文档集合中搜索答案。例如，利用多语言向量检索器在多语维基百科上查找与用户问题匹配的段落。其优点是**检索范围广**，能够覆盖用户语言和英文等多种语言的信息来源。实际研究表明，先进的多语言检索模型（如BGE-m3）在跨语言检索上已达到较高水准，甚至优于将查询翻译后用英文检索的效果。不过，由于不同语言资料表述方式不同，直接多语言检索可能导致**证据内容不一致**或质量参差不齐，给后续答案生成带来挑战。

* **跨语言RAG融合 (CrossRAG)**：这是为解决上述问题提出的改进架构。其流程是**多语检索 + 文档翻译 + 统一生成**：先从多语言语料中检索，再将获得的不同语言证据**翻译成统一语言**（如英文），最后基于统一后的证据用目标语言生成答案。Ranaldi等人的实验表明，该方法显著提升了跨语言知识密集型任务的性能，在高资源和低资源语言上均有改善。CrossRAG在保证多语言覆盖的同时，降低了因证据语言差异导致的生成困扰，被证明是目前**效果最优**的跨语言RAG策略之一。

* **mRAG多语言基线系统**：Chirkova等人在2024年的工作中构建了一个**强基线多语言RAG管线**。他们采用预训练的高质量多语言检索器（如BGE-m3）和生成模型（如Command-**R** 35B），并通过**提示工程**来确保模型按用户语言作答。该系统在包含13种语言的开放域问答数据集上进行了评估，是我们对比实验的重要参考。其特点：1）使用**多语言维基百科**作为知识源，证明了在大多数情况下多语言检索优于仅检索英语；2）强调了评价指标和模型输出在多语言环境下的特殊性，例如需要统一度量不同语言的拼写差异；3）发现了现存问题，如模型有时会**代码混用**（输出掺杂英文）或生成不流畅等。他们通过在提示中显式要求“用用户语言回答”部分缓解了回答跑偏的问题。该基线为我们提供了实践经验，例如多语言检索器的选择和语言一致性的控制方法。

* **XRAG跨语言RAG基准**：虽然XRAG是一个评测**数据集/基准**而非系统，但值得一提。Liu等人构建了XRAG数据集，用于评测LLM在**跨语言RAG**情境下的能力。XRAG涵盖了两类场景：一是*单语检索*（仅英文知识库，用户用非英语言提问），二是*多语检索*（英文+用户语言混合知识库）。他们用多个大型模型在XRAG上测试，揭示出两大挑战：**(1)** 在单语检索场景下，模型常常无法用正确语言回答用户（即回答不是用用户语言）；**(2)** 在多语检索场景下，主要难点在于**跨语言整合推理**，而非生成非英语文本本身。XRAG为我们提供了一个综合评测跨语言RAG性能的基准，我们可据此设计实验，检验本研究系统是否在上述两个方面有所改进。

*(以上列举的系统/架构及论文链接，均为2023年以后发表的代表性成果。它们将作为本课题方案的对比基础，为我们的创新提供思路和评价参照。)*

## 预期的实证分析方法

为验证本研究提出的多语言RAG系统的有效性，我们将进行严格的实证评估。具体方案包括数据集选择、实验设置和评估指标三部分：

**数据集选择：**考虑使用**开放域多语言问答**基准数据集进行评测，以保证实验具有权威性和通用性。首先，选取 **MKQA** 数据集（Multilingual Knowledge Questions & Answers）。MKQA包含1万个问答对，覆盖26种语言，对齐同一语义内容。每道题在不同语言都有对应问题，答案采用与语言无关的表示，使不同语言的结果可直接比较。它提供了广泛的语言分布，是评估系统跨语言一致性和准确度的理想选择。其次，使用 **XOR-TyDi QA** 数据集。XOR-TyDi QA源自TyDi QA，但针对跨语言开放域问答设计：问题为非英语，答案需要从英文维基百科中检索，适合测试**单语库跨语言检索**场景。该数据集包含多种语言的问题，可用于评估我们系统在仅英文知识源情况下处理不同语言提问的能力。最后，如XRAG基准数据发布，我们亦考虑在该数据集上测试，以获取更复杂场景下的表现；否则，将采用作者提供的XRAG评测方案或构造类似测试集合。整体上，这些数据集覆盖了**多语言->多语言**和**多语言->英文**两种典型场景，能够全面检验系统性能。

\*\*实验设定：\*\*我们将实现数个系统配置进行对比实验：

1. \*\*Translate-Query Baseline：\*\*将问题翻译为英文，用英文检索+英文资料+再将答案翻译回用户语言的流程（模拟Translate-RAG策略）。
2. \*\*Multilingual Retrieval Baseline：\*\*直接用多语言检索模型在多语言知识库中检索，输入原语言问题和证据给生成模型（模拟MultiRAG策略）。
3. **Proposed Cross-lingual System：**使用我们设计的**检索融合+证据翻译+多语生成**的完整架构。
4. \*\*Ablation Variants：\*\*为分析各模块贡献，我们将构造一些变体实验。例如取消证据翻译步骤（直接让生成器处理多语言证据）、或取消双通道检索仅用单一路径等，观察性能变化。这有助于证明我们方案中各关键组件（如证据翻译、提示控制）的作用。

所有系统将利用相同的底层组件（如相同的多语言检索模型和基础生成模型）以确保公平。例如，检索部分可统一采用**BGE-m3多语言向量检索模型**构建英文及多语混合索引，这个模型在多语言检索基准上表现领先。生成模型则选用支持多语言的LLM，如开源的mT5或Fine-tuned LLaMA-2，多系统统一以相同参数设定运行。对于Translate-Query基线，会在生成前后各加一次翻译步骤（提问翻译和答案回译），尽量保证除了跨语言处理方式外，其它因素一致。

我们将针对**高资源语言**（如日语、中文、法语）和**低资源语言**（如泰语、芬兰语等）分别评估。在每种语言上，比较不同系统在回答准确率和语言质量方面的差异。这不仅测试整体优劣，还能体现我们系统对不同资源级别语言的适应性。此外，在XOR-TyDi等单语库场景，我们的系统应表现出比简单基线更好的回答语言控制和准确率；在MKQA等多语库场景，我们预期多语言融合策略能提高答案覆盖度和正确性。实验将采用**统计显著性检验**（如t检验或Bootstrap方法）验证不同方法性能差异是否显著，以确保结论可靠。

**评估指标：**我们将从**检索**和**回答生成**两个层面设立评估指标：

* *检索性能：*衡量检索模块返回的证据质量。采用**Recall\@K**（如K=5或10）指标，查看前K个检索片段中是否包含正确答案所需信息。对于跨语言情境，还会统计英文来源与本地语言来源证据的占比，以分析不同策略对多语言信息获取的影响。如果有标准的相关性标注（如XRAG提供的文档相关性标注），也可计算\*\*平均准确率（Precision）\*\*或*MRR*（平均折返率）等指标评价检索排序效果。

* \*答案准确性：\*这是核心评价，检验系统给出的最终答案是否正确完整。**准确率**将用**Exact Match (EM)**和**F1分数**等指标衡量：EM表示生成答案与参考答案完全匹配的比例，F1则考虑部分匹配（对答案包含多个要素的情况计算精确率与召回率的调和平均）。在开放问答中，如果答案是一个名称或短语，我们主要看EM；若是句子描述，则F1更具参考价值。此外参考**BLEU**或**ROUGE**等机器翻译/摘要评价指标，用于评估生成答案与参考答案在字词层面的相似度（特别是在答案可能用不同表述的情况下）。例如，当答案是一个句子时，可计算BLEU来衡量模型回答与标准答案表述的接近程度（BLEU最初用于评价翻译质量）。需要注意，多语言环境下**评估需适当规范化**：例如忽略大小写、去除日文中的空格差异，或统一繁简体，才能公平比较答案文本。

* \*语言一致性：\*即答案语言与用户提问语言的一致程度。这方面，我们定义**语言正确率**指标：统计模型回答完全使用了期望语言的比例，理想情况下应达到100%。具体实现上，可以借助语言检测工具自动判断答案使用的语言。如果出现回答中夹杂其他语言词汇的情况，记为不一致。我们将重点比较各方法在这项指标上的差异，验证我们的系统是否成功消除了代码混用问题。例如，在Sorokin等早期系统中曾出现模型回答部分内容跑成英文的情况，我们的改进是否杜绝了此类现象。

* \*答案质量和其他：\*包括**流畅性**和**自然度**，以及**事实一致性**等主观质量指标。这些可能难以用客观打分直接衡量，因此我们考虑引入**人工评估**或**LLM评估**作为辅助。比如随机抽取若干不同语言的问题，由双语人员检查各系统答案的流畅性和是否符合提问语言习惯。在事实一致性方面，可以通过将模型答案与检索证据比对，确保答案中的事实在证据中确有支持。如果条件允许，可借助GPT-4等高性能模型来对比两个答案的正确性或进行投票评估（类似XRAG中采用多个LLM评审答案的方法），但本科层面的论文可能主要以自动指标为主，人工评估作为补充说明。

需要强调的是，多语言评估可能遇到一些特殊挑战。例如，不同语言对同一实体可能有不同译名或拼写，直接的文本匹配会低估正确率。为此，我们将对评估流程进行调整：例如，对专有名词建立跨语言映射（通过维基百科或翻译获取等价说法），在计算EM/F1时将模型输出和参考答案都规范化处理再比较。这种做法参考了Chirkova等工作的建议，即**调整评价指标以适应多语言**场景。通过一系列自动和必要的人工指标，我们将全面衡量系统在**正确性**、**语言控制**和**跨语言泛化**方面的表现。

## 推荐的论文结构

按照日本大学学部毕业论文的一般格式，我们建议最终论文采用如下结构（约30页），以清晰展现研究内容：

1. **绪论（序論）**：介绍课题背景和动机。阐述多语言RAG的重要性，以及目前存在的跨语言问答困难。引用相关报告和实例说明现实需求，明确本研究的目标和意义。绪论还应概要描述论文结构。

2. **相关研究（関連研究）**：综述与本课题相关的研究工作，包括开放域问答、RAG框架，以及多语言问答的最新进展。重点梳理当前已有的多语言RAG架构和方法，例如Translate-Query方案、MultiRAG、CrossRAG等，并比较它们的优缺点和适用情境。也可提及跨语言信息检索（CLIR）领域的方法和成果，以及评测数据集（如MKQA、XOR-QA）的情况。通过相关研究的分析，凸显现有工作的不足之处（如回答语言不一致、跨语言推理弱等），引出本研究的创新之处。

3. **研究方法与系统设计**：详细描述本研究提出的多语言RAG系统架构和技术方案。这一部分相当于**提案手法**，包括系统整体流程图以及各模块功能说明。例如，检索模块如何并行多语言检索与翻译检索，证据如何转换统一语言，生成模块如何利用提示控制输出语言等，都需逐点说明，并阐明这样设计的理论依据。可以将第2章综述的参考方案与本方法逐一对比，强调我们的改进（例如如何解决多语言检索内容不一致的问题，如何避免代码混用）。如果有实现上的细节（模型选型、训练微调策略等），也在此交代。此章节应突出**设计要点与创新**，让读者清楚本研究的新颖性所在。

4. **实证分析（実証分析）**：对应实验与结果部分。首先介绍实验设置，包括数据集选取和处理方式，评价指标定义，实验环境等（可参考上节“预期的实证分析方法”所述内容）。接着分语言或分方案报告实验结果，例如用表格展示各对比系统在MKQA各语言上的EM/F1、在XOR-QA上的表现、以及语言一致性指标的统计。需要对结果进行分析讨论：**验证假设**是否成立，本研究系统是否显著优于基线方法，在哪些语言或场景优势明显。的研究已表明多语言检索在多数情况下提升了性能，我们的实验可进一步佐证这一点，并考察我们的跨语言融合策略是否带来额外收益。对于异常情况（如某些语言上改进有限，或某指标上略有下降），也要给予解释，例如可能因为翻译误差或数据集特性导致。此部分还可加入案例分析，用具体问答实例比较不同系统输出，以质化说明我们方法如何更好地回答了问题。通过定量加定性的实证分析，证明本课题的**可行性和有效性**。

5. **考察（考察）**：在讨论章节中，针对实验发现进行更深层次的考察和推理。可以讨论本系统的适用范围和局限：例如当用户问题含混或知识库无答案时，多语言RAG是否有劣势？我们的方法对计算资源要求如何，是否适合实际部署？还应反思研究中存在的不足，比如翻译模块引入的新误差、评估指标可能无法捕捉的方面等。同时，将本研究放置于更广阔的背景下考量：多语言RAG的社会影响（能帮助不同语言使用者更平等地获取知识），潜在的伦理问题（如翻译偏差造成的误导）等。考察部分还可以结合相关研究，对有待进一步解决的问题提出展望。例如，是否可以引入交互式反馈机制让用户纠正答案语言，或者未来可以尝试端到端训练一个跨语言RAG模型而无需显式翻译步骤等。通过这些讨论，表明作者对课题有全面深入的理解。

6. **结论（結論）**：总结全文，重申本研究解决的问题、提出的方案及其效果。概括本研究多语言RAG架构如何实现了跨语言检索与生成性能的优化，与现有方法相比取得了怎样的改进。提炼出主要贡献点，例如“提出了XX方法显著改善了回答语言一致性”等，用定量结果支持。的结果可用于强调我们方法在高低资源语言上的普适性。最后指出本研究的**学术价值和应用前景**：学术上填补了多语言RAG领域的空白，应用上可用于构建多语言智能问答系统等。同时也可提出未来工作的方向，例如扩展更多语言、引入多模态信息等，为后继研究者提供思路。结论应当简洁有力，为论文划下圆满句点。

以上结构遵循日文论文写作规范，从序论到结論脉络清晰，涵盖背景、相关研究、方法、实验、讨论和总结各环节。全文预计30页左右，符合学部毕业论文篇幅要求。通过这样的布局，读者可以循序渐进地理解**多语言RAG系统的跨语言性能优化与对比分析**这一课题，从理论动机到实证结果一目了然。整篇论文将体现出严谨的学术态度和工程实现的可行性，使本课题方案具有较高的学术价值和应用潜力。各章内容衔接紧密，既有对前人工作的继承对比，又有我们创新方案的详尽阐述，最终通过数据和分析支撑起结论，完成对研究问题的全面解答。
